{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpufit/Comet/blob/master/Colab_notebooks/COMET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://raw.githubusercontent.com/gpufit/Comet/master/Python_interface/resources/comet_logo_small.png)\n",
        "\n",
        "#Cost-function Optimized Maximal overlap drift EsTimation\n",
        "\n",
        "COMET is a software package designed to correct drift in single molecule localization microscopy (SMLM) datasets with optimal spatial and temporal resolution.\n",
        "\n",
        "Check out the Comet [github repository](https://github.com/gpufit/Comet) for  detailed information.\n",
        "\n",
        "Note: to try Comet with a simple test dataset, a sample dataset is availble [here](https://raw.githubusercontent.com/gpufit/Comet/master/test_dataset/test_dataset.csv).\n"
      ],
      "metadata": {
        "id": "yniUwNRUY_wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use the Colab notebook\n",
        "\n",
        "## Step 1: Environment setup (run once)\n",
        "\n",
        "1.   Ensure that you are logged into your Google account.\n",
        "2.   Select from the dropdown menu: File -> Save a Copy in Drive.\n",
        "3.   Select from the dropdown menu: Edit -> Notebook Settings and ensure that GPU hardware acceleration is selected.\n",
        "4.   Run the next 3 blocks of code (below) only once, by clicking the \"Play\" buttons one at a time, to finish setting up the remote hardware and software environment.\n"
      ],
      "metadata": {
        "id": "2H3H9JiFbLco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.1 Install Numba\n",
        "!uv pip install -q --system numba-cuda==0.4.0"
      ],
      "metadata": {
        "id": "QeCgi7pTzdge",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.2 Import Numba\n",
        "from numba import config\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1"
      ],
      "metadata": {
        "id": "_6bNIn-Szlf9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.3 Function definitions\n",
        "## Function definitions\n",
        "import warnings\n",
        "import numpy as np\n",
        "from typing import Optional, Dict\n",
        "from dataclasses import dataclass\n",
        "from numba import cuda\n",
        "import math\n",
        "from scipy.optimize import minimize\n",
        "import warnings\n",
        "import time\n",
        "from scipy.spatial import cKDTree\n",
        "import h5py\n",
        "from scipy.ndimage import convolve\n",
        "#### Segmentation\n",
        "@dataclass\n",
        "class SegmentationResult:\n",
        "    loc_segments: np.ndarray       # for each localization, which segment it belongs to\n",
        "    loc_valid: np.ndarray          # boolean mask (True if that localization is “kept”)\n",
        "    center_frames: np.ndarray      # average frame‐index of each segment\n",
        "    n_segments: int                # total number of segments\n",
        "    out_dict: Optional[Dict] = None\n",
        "\n",
        "def _group_by_frame(loc_frames: np.ndarray):\n",
        "    \"\"\"Returns (unique_frames, frame_to_indices)\"\"\"\n",
        "    sort_idx = np.argsort(loc_frames)\n",
        "    sorted_frames = loc_frames[sort_idx]\n",
        "    unique_frames, start_idx, counts = np.unique(\n",
        "        sorted_frames, return_index=True, return_counts=True\n",
        "    )\n",
        "    frame_to_indices = {\n",
        "        frame: sort_idx[start : start + count]\n",
        "        for frame, start, count in zip(unique_frames, start_idx, counts)\n",
        "    }\n",
        "    return unique_frames, frame_to_indices\n",
        "\n",
        "def segment_by_num_locs_per_window(\n",
        "    loc_frames: np.ndarray,\n",
        "    min_n_locs_per_window: int,\n",
        "    max_locs_per_segment: Optional[int] = None,\n",
        "    return_param_dict: bool = False,\n",
        ") -> SegmentationResult:\n",
        "    \"\"\"\n",
        "    Segments by collecting at least min_n_locs_per_window localizations per segment.\n",
        "    Once that threshold is met (and there are still enough locs left for another segment),\n",
        "    a new segment is created.\n",
        "    \"\"\"\n",
        "    loc_frames = np.asarray(loc_frames, dtype=int)\n",
        "    n_locs = len(loc_frames)\n",
        "\n",
        "    unique_frames, frame_to_indices = _group_by_frame(loc_frames)\n",
        "    loc_segments = np.full(n_locs, -1, dtype=int)\n",
        "    segment_counter = 0\n",
        "    n_locs_in_current_segment = 0\n",
        "    current_segment_indices = []\n",
        "    start_frames, end_frames, locs_per_segment = [], [], []\n",
        "\n",
        "    for i, frame in enumerate(unique_frames):\n",
        "        indices = frame_to_indices[frame]\n",
        "        n_locs_this_frame = len(indices)\n",
        "        remaining_locs = n_locs - (len(current_segment_indices) + n_locs_this_frame)\n",
        "\n",
        "        # If adding this frame meets or exceeds the threshold AND\n",
        "        # there are still >= min_n_locs_per_window locs left (or we are at the last frame),\n",
        "        # then finalize the current segment and start a new one.\n",
        "        if (\n",
        "            (n_locs_in_current_segment + n_locs_this_frame >= min_n_locs_per_window)\n",
        "            and (remaining_locs >= min_n_locs_per_window or i == len(unique_frames) - 1)\n",
        "        ):\n",
        "            current_segment_indices.extend(indices)\n",
        "            n_locs_in_current_segment += n_locs_this_frame\n",
        "\n",
        "            loc_segments[current_segment_indices] = segment_counter\n",
        "            start_frames.append(loc_frames[current_segment_indices[0]])\n",
        "            end_frames.append(loc_frames[current_segment_indices[-1]])\n",
        "            locs_per_segment.append(len(current_segment_indices))\n",
        "\n",
        "            segment_counter += 1\n",
        "            current_segment_indices = []\n",
        "            n_locs_in_current_segment = 0\n",
        "        else:\n",
        "            current_segment_indices.extend(indices)\n",
        "            n_locs_in_current_segment += n_locs_this_frame\n",
        "\n",
        "    n_segments = segment_counter\n",
        "    center_frames = np.zeros(n_segments)\n",
        "    loc_valid = np.zeros(n_locs, dtype=bool)\n",
        "\n",
        "    for seg_id in range(n_segments):\n",
        "        segment_indices = np.where(loc_segments == seg_id)[0]\n",
        "        if max_locs_per_segment and len(segment_indices) > max_locs_per_segment:\n",
        "            selected = np.random.choice(segment_indices, max_locs_per_segment, replace=False)\n",
        "        else:\n",
        "            selected = segment_indices\n",
        "        loc_valid[selected] = True\n",
        "        center_frames[seg_id] = np.mean(loc_frames[selected])\n",
        "\n",
        "    out_dict = None\n",
        "    if return_param_dict:\n",
        "        n_locs_valid = loc_valid.sum()\n",
        "        out_dict = {\n",
        "            \"n_segments\": n_segments,\n",
        "            \"min_n_locs_per_window\": min_n_locs_per_window,\n",
        "            \"frames_per_window\": -1,\n",
        "            \"start_frames\": np.array(start_frames),\n",
        "            \"end_frames\": np.array(end_frames),\n",
        "            \"locs_per_segment\": np.array(locs_per_segment),\n",
        "            \"n_locs\": n_locs,\n",
        "            \"n_locs_valid\": n_locs_valid,\n",
        "            \"n_locs_invalid\": n_locs - n_locs_valid,\n",
        "            \"center_frames\": center_frames,\n",
        "        }\n",
        "\n",
        "    return SegmentationResult(loc_segments, loc_valid, center_frames, n_segments, out_dict)\n",
        "\n",
        "def segment_by_frame_windows(\n",
        "    loc_frames: np.ndarray,\n",
        "    n_frames_per_window: int,\n",
        "    max_locs_per_segment: Optional[int] = None,\n",
        "    return_param_dict: bool = False,\n",
        ") -> SegmentationResult:\n",
        "    \"\"\"\n",
        "    Splits localization data into fixed‐size windows of N frames.\n",
        "    All localizations in those frames are grouped into one segment.\n",
        "    \"\"\"\n",
        "    loc_frames = np.asarray(loc_frames, dtype=int)\n",
        "    frames, frame_to_indices = _group_by_frame(loc_frames)\n",
        "    n_locs = len(loc_frames)\n",
        "    n_segments = int(np.ceil(len(frames) / n_frames_per_window))\n",
        "\n",
        "    loc_segments = np.zeros(n_locs, dtype=int)\n",
        "    center_frames = np.zeros(n_segments)\n",
        "    loc_valid = np.ones(n_locs, dtype=bool)\n",
        "    start_frames, end_frames, locs_per_segment = [], [], []\n",
        "\n",
        "    for seg_id in range(n_segments):\n",
        "        frame_window = frames[seg_id * n_frames_per_window : (seg_id + 1) * n_frames_per_window]\n",
        "        # gather all indices for these frames\n",
        "        indices = np.concatenate(\n",
        "            [frame_to_indices[f] for f in frame_window if f in frame_to_indices]\n",
        "        )\n",
        "        if len(indices) == 0:\n",
        "            continue\n",
        "        loc_segments[indices] = seg_id\n",
        "        start_frames.append(frame_window[0])\n",
        "        end_frames.append(frame_window[-1])\n",
        "        center_frames[seg_id] = np.mean(loc_frames[indices])\n",
        "        locs_per_segment.append(len(indices))\n",
        "\n",
        "        if max_locs_per_segment and len(indices) > max_locs_per_segment:\n",
        "            mask = np.ones(len(indices), dtype=bool)\n",
        "            mask[np.random.choice(len(indices), len(indices) - max_locs_per_segment, replace=False)] = False\n",
        "            loc_valid[indices[~mask]] = False\n",
        "\n",
        "    out_dict = None\n",
        "    if return_param_dict:\n",
        "        n_locs_valid = loc_valid.sum()\n",
        "        out_dict = {\n",
        "            \"n_segments\": n_segments,\n",
        "            \"min_n_locs_per_window\": -1,\n",
        "            \"frames_per_window\": n_frames_per_window,\n",
        "            \"start_frames\": np.array(start_frames),\n",
        "            \"end_frames\": np.array(end_frames),\n",
        "            \"locs_per_segment\": np.array(locs_per_segment),\n",
        "            \"n_locs\": n_locs,\n",
        "            \"n_locs_valid\": n_locs_valid,\n",
        "            \"n_locs_invalid\": n_locs - n_locs_valid,\n",
        "            \"center_frames\": center_frames,\n",
        "        }\n",
        "\n",
        "    return SegmentationResult(loc_segments, loc_valid, center_frames, n_segments, out_dict)\n",
        "\n",
        "def segment_by_num_windows(\n",
        "    loc_frames: np.ndarray,\n",
        "    n_windows: int,\n",
        "    max_locs_per_segment: Optional[int] = None,\n",
        "    return_param_dict: bool = False,\n",
        ") -> SegmentationResult:\n",
        "    \"\"\"\n",
        "    Converts a fixed number of windows into an equivalent minimum locs per window,\n",
        "    then calls `segment_by_num_locs_per_window`.\n",
        "    \"\"\"\n",
        "    n_locs = len(loc_frames)\n",
        "    n_locs_per_window = int(np.ceil(n_locs / n_windows))\n",
        "    return segment_by_num_locs_per_window(\n",
        "        loc_frames, n_locs_per_window, max_locs_per_segment, return_param_dict\n",
        "    )\n",
        "\n",
        "def segmentation_wrapper(\n",
        "    loc_frames: np.ndarray,\n",
        "    segmentation_var: int,\n",
        "    segmentation_mode: int = 2,\n",
        "    max_locs_per_segment: Optional[int] = None,\n",
        "    return_param_dict: bool = False,\n",
        ") -> SegmentationResult:\n",
        "    \"\"\"\n",
        "    Dispatch function that selects segmentation method:\n",
        "      0 → fixed number of windows\n",
        "      1 → fixed number of localizations per window\n",
        "      2 → fixed number of frames per window (default)\n",
        "    \"\"\"\n",
        "    if segmentation_mode == 0:\n",
        "        return segment_by_num_windows(\n",
        "            loc_frames, segmentation_var, max_locs_per_segment, return_param_dict\n",
        "        )\n",
        "    elif segmentation_mode == 1:\n",
        "        return segment_by_num_locs_per_window(\n",
        "            loc_frames, segmentation_var, max_locs_per_segment, return_param_dict\n",
        "        )\n",
        "    else:\n",
        "        return segment_by_frame_windows(\n",
        "            loc_frames, segmentation_var, max_locs_per_segment, return_param_dict\n",
        "        )\n",
        "\n",
        "### Finding Pairs\n",
        "def pair_indices_kdtree(coordinates, distance):\n",
        "    tree = cKDTree(coordinates)\n",
        "    while True:\n",
        "        try:\n",
        "            pairs = tree.query_pairs(r=distance, output_type='ndarray')\n",
        "            break\n",
        "        except MemoryError:\n",
        "            distance *= 0.8\n",
        "            print(f\"[pair_indices_kdtree] Reducing distance to {distance:.2f} due to memory error.\")\n",
        "\n",
        "    print(f\"[pair_indices_kdtree] Found {len(pairs):,} pairs\")\n",
        "    return np.ascontiguousarray(pairs[:, 0], dtype=np.int32), np.ascontiguousarray(pairs[:, 1], dtype=np.int32)\n",
        "\n",
        "### Drift correction\n",
        "\n",
        "@cuda.jit\n",
        "def cost_function_full_3d_chunked(d_locs_time, start_idx, chunk_size, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val,\n",
        "                                  d_val_sum, d_deri, d_locs_coords, mu):\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.blockIdx.x\n",
        "    bw = cuda.blockDim.x\n",
        "    pos = tx + ty * bw\n",
        "\n",
        "    if pos < chunk_size:\n",
        "        i = d_idx_i[pos + start_idx]\n",
        "        j = d_idx_j[pos + start_idx]\n",
        "\n",
        "        ti = d_locs_time[i]\n",
        "        tj = d_locs_time[j]\n",
        "\n",
        "        dx = (d_locs_coords[i, 0] - mu[ti, 0]) - (d_locs_coords[j, 0] - mu[tj, 0])\n",
        "        dy = (d_locs_coords[i, 1] - mu[ti, 1]) - (d_locs_coords[j, 1] - mu[tj, 1])\n",
        "        dz = (d_locs_coords[i, 2] - mu[ti, 2]) - (d_locs_coords[j, 2] - mu[tj, 2])\n",
        "        sigma_sq = (2 * d_sigma * d_sigma_factor) ** 2\n",
        "\n",
        "        diff_sq = dx * dx + dy * dy + dz * dz\n",
        "        val = 1 / (d_sigma * d_sigma_factor) * math.exp(-diff_sq / sigma_sq)\n",
        "        d_val[pos] = val\n",
        "\n",
        "        # Update derivatives\n",
        "        cuda.atomic.add(d_deri, (tj, 0), 2 * val * (d_locs_coords[j, 0] - d_locs_coords[i, 0] + mu[ti, 0] - mu[tj, 0]) / sigma_sq)\n",
        "        cuda.atomic.add(d_deri, (tj, 1), 2 * val * (d_locs_coords[j, 1] - d_locs_coords[i, 1] + mu[ti, 1] - mu[tj, 1]) / sigma_sq)\n",
        "        cuda.atomic.add(d_deri, (tj, 2), 2 * val * (d_locs_coords[j, 2] - d_locs_coords[i, 2] + mu[ti, 2] - mu[tj, 2]) / sigma_sq)\n",
        "\n",
        "        cuda.atomic.add(d_deri, (ti, 0), 2 * val * (d_locs_coords[i, 0] - d_locs_coords[j, 0] + mu[tj, 0] - mu[ti, 0]) / sigma_sq)\n",
        "        cuda.atomic.add(d_deri, (ti, 1), 2 * val * (d_locs_coords[i, 1] - d_locs_coords[j, 1] + mu[tj, 1] - mu[ti, 1]) / sigma_sq)\n",
        "        cuda.atomic.add(d_deri, (ti, 2), 2 * val * (d_locs_coords[i, 2] - d_locs_coords[j, 2] + mu[tj, 2] - mu[ti, 2]) / sigma_sq)\n",
        "\n",
        "        cuda.atomic.add(d_val_sum, 0, val)\n",
        "        d_val[pos] = 0\n",
        "\n",
        "\n",
        "# Interface between the Python code and the CUDA kernel, mainly for chunking the data to avoid memory issues\n",
        "def cuda_wrapper_chunked(mu, d_locs_coords, d_locs_time, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val, d_deri,\n",
        "                         chunk_size):\n",
        "    val_total = 0\n",
        "    d_val_sum = cuda.to_device(np.zeros(1, dtype=np.float64))\n",
        "    mu_dev = cuda.to_device(np.asarray(mu.reshape(int(mu.size / 3), 3), dtype=np.float64))\n",
        "\n",
        "    n_chunks = int(np.ceil(d_idx_i.size / chunk_size))\n",
        "    threadsperblock = 128\n",
        "\n",
        "    for i in range(n_chunks - 1):\n",
        "        idc_start = i*chunk_size\n",
        "        blockspergrid = (chunk_size + (threadsperblock - 1)) // threadsperblock\n",
        "        cost_function_full_3d_chunked[blockspergrid, threadsperblock](\n",
        "            d_locs_time, idc_start, chunk_size, d_idx_i, d_idx_j,\n",
        "            d_sigma, d_sigma_factor, d_val, d_val_sum, d_deri, d_locs_coords, mu_dev\n",
        "        )\n",
        "        val_total += d_val_sum.copy_to_host()\n",
        "\n",
        "    # Final chunk\n",
        "    n_remaining = d_idx_i.size - (n_chunks - 1) * chunk_size\n",
        "    idc_start = (n_chunks - 1) * chunk_size\n",
        "    blockspergrid = (n_remaining + (threadsperblock - 1)) // threadsperblock\n",
        "    cost_function_full_3d_chunked[blockspergrid, threadsperblock](\n",
        "        d_locs_time, idc_start, n_remaining, d_idx_i, d_idx_j,\n",
        "        d_sigma, d_sigma_factor, d_val, d_val_sum, d_deri, d_locs_coords, mu_dev\n",
        "    )\n",
        "    val_total += d_val_sum.copy_to_host()\n",
        "    deri = d_deri.copy_to_host()\n",
        "    d_deri[:] = 0\n",
        "\n",
        "    return -np.nansum(val_total), -deri.flatten()\n",
        "\n",
        "def optimize_3d_chunked_better_moving_avg_kd(n_segments, locs_nm, sigma_nm=30, drift_max_nm=300,\n",
        "                                             target_sigma_nm=30, display_steps=False,\n",
        "                                             save_intermdiate_results=False,\n",
        "                                             boxcar_width=3, drift_max_bound_factor=2,\n",
        "                                             force_cpu=False, return_calc_time=False):\n",
        "\n",
        "    intermediate_results_filehandle = None\n",
        "    sigma_factor = 1.0\n",
        "\n",
        "    # Find spatially close localization pairs\n",
        "    idx_i, idx_j = pair_indices_kdtree(locs_nm[:, :3], drift_max_nm)\n",
        "\n",
        "    # Extract coordinate + time arrays, convert to device if CUDA\n",
        "    coords = locs_nm[:, :3].astype(np.float32).copy()\n",
        "    times = locs_nm[:, 3].astype(np.int32).copy()\n",
        "\n",
        "    chunk_size = int(1E8)  # 1E7\n",
        "\n",
        "    if not force_cpu:\n",
        "        d_coords = cuda.to_device(coords)\n",
        "        d_times = cuda.to_device(times)\n",
        "        if len(idx_i) * 4 > 2e9:\n",
        "            # Use mapped memory if index arrays are large\n",
        "            print(\"Large index arrays — using mapped memory.\")\n",
        "            d_idx_i = cuda.mapped_array_like(idx_i.astype(np.int32), wc=True)\n",
        "            d_idx_j = cuda.mapped_array_like(idx_j.astype(np.int32), wc=True)\n",
        "            d_idx_i[:] = idx_i\n",
        "            d_idx_j[:] = idx_j\n",
        "        else:\n",
        "            d_idx_i = cuda.to_device(idx_i.astype(np.int32))\n",
        "            d_idx_j = cuda.to_device(idx_j.astype(np.int32))\n",
        "        # Preallocate device arrays\n",
        "        d_sigma = np.float64(sigma_nm)\n",
        "        d_val = cuda.to_device(np.zeros(chunk_size))\n",
        "        d_deri = cuda.to_device(np.zeros((n_segments, 3), dtype=np.float64))\n",
        "    else:\n",
        "        # Fallback: CPU arrays\n",
        "        d_coords = coords\n",
        "        d_times = times\n",
        "        d_sigma = sigma_nm\n",
        "        d_idx_i, d_idx_j = idx_i, idx_j\n",
        "        d_val = np.zeros(len(idx_i), dtype=np.float64)\n",
        "        d_deri = np.zeros((n_segments, 3), dtype=np.float64)\n",
        "\n",
        "    # Initial drift estimate + bounds\n",
        "    drift_est = np.zeros(n_segments * 3)\n",
        "    bounds = [(-drift_max_nm * drift_max_bound_factor, drift_max_nm * drift_max_bound_factor)] * (3 * n_segments)\n",
        "\n",
        "    drift_est_gradient = np.inf\n",
        "    fails = 0\n",
        "    done = False\n",
        "    itr_counter = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    wrapper = cuda_wrapper_chunked_cpu if force_cpu else cuda_wrapper_chunked\n",
        "    print(\"Using CPU wrapper for optimization.\" if force_cpu else \"Using CUDA wrapper for optimization.\")\n",
        "    print(f\"Number of pairs: {len(idx_i)}\")\n",
        "\n",
        "    while not done:\n",
        "        d_sigma_factor = np.float64(sigma_factor)\n",
        "        # Apply boxcar smoothing to current estimate\n",
        "        tmp = drift_est.reshape((-1, 3))\n",
        "        for i in range(3):\n",
        "            tmp[:, i] = convolve(tmp[:, i], np.ones(boxcar_width) / boxcar_width)\n",
        "        drift_est = tmp.flatten()\n",
        "\n",
        "        # Run L-BFGS-B optimization step\n",
        "        result = minimize(wrapper, drift_est, method='L-BFGS-B',\n",
        "                          args=(\n",
        "                              d_coords, d_times, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val, d_deri, chunk_size),\n",
        "                          jac=True, bounds=bounds,\n",
        "                          options={'disp': display_steps, 'gtol': 1E-5, 'ftol': 1E3 * np.finfo(float).eps,\n",
        "                                   'maxls': 40})\n",
        "        itr_counter += 1\n",
        "        print(f\"Iteration {itr_counter}: status = {result.status}, success = {result.success}\")\n",
        "        print(f\"  current sigma: {np.round(sigma_nm * sigma_factor, 2)} nm\")\n",
        "\n",
        "        # Optionally save intermediate result to HDF5\n",
        "        if save_intermdiate_results:\n",
        "            intermediate_results_filehandle = save_intermediate_results_wrapper(drift_est_nm=result['x'],\n",
        "                                                                                locs_nm=locs_nm,\n",
        "                                                                                sigma_nm=sigma_nm,\n",
        "                                                                                sigma_factor=sigma_factor,\n",
        "                                                                                itr_counter=itr_counter, fails=fails,\n",
        "                                                                                filehandle=intermediate_results_filehandle)\n",
        "        # Update if successful\n",
        "        if result.success:\n",
        "            delta = np.median((result.x - drift_est) ** 2)\n",
        "            print(f\"  drift estimate gradient: {delta}\")\n",
        "            print(f\"  previous gradient: {drift_est_gradient}\")\n",
        "            # Check convergence\n",
        "            if (delta > drift_est_gradient or sigma_nm * sigma_factor <= 1.0) and sigma_nm * sigma_factor <= target_sigma_nm:\n",
        "                done = True\n",
        "                calc_time = time.time() - start_time\n",
        "                print(f\"Optimization completed in {calc_time:.2f} s\")\n",
        "            else:\n",
        "                sigma_factor /= 1.5\n",
        "                drift_est_gradient = delta\n",
        "                drift_est = result.x\n",
        "        else:\n",
        "            fails += 1\n",
        "            if fails > 2:\n",
        "                sigma_factor *= 2\n",
        "                print(\"Restarting with larger sigma_factor\")\n",
        "            if fails > 5:\n",
        "                raise RuntimeError(\"L-BFGS-B Optimization failed after multiple retries\")\n",
        "\n",
        "    if return_calc_time:\n",
        "        return drift_est, time.time() - start_time, itr_counter\n",
        "    else:\n",
        "        return drift_est\n",
        "\n",
        "\n",
        "def comet_run_kd(dataset, segmentation_result, max_locs_per_segment=None,\n",
        "                 initial_sigma_nm=None, gt_drift=None, display=False, return_corrected_locs=False,\n",
        "                 max_drift=None, target_sigma_nm=10, boxcar_width=1, drift_max_bound_factor=2,\n",
        "                 save_corrected_locs=False, save_filepath=None, save_intermediate_results=False,\n",
        "                 interpolation_method='cubic', force_cpu=False, min_max_frames=None):\n",
        "\n",
        "    loc_frames = dataset[:, -1]\n",
        "\n",
        "\n",
        "    # Apply segment IDs and mask out invalid localizations\n",
        "    sorted_dataset = dataset.copy()\n",
        "    sorted_dataset[:, -1] = segmentation_result.loc_segments\n",
        "    sorted_dataset = sorted_dataset[segmentation_result.loc_valid]\n",
        "    loc_frames = loc_frames[segmentation_result.loc_valid]\n",
        "\n",
        "    # Set default max drift if not provided\n",
        "    if initial_sigma_nm is None:\n",
        "        initial_sigma_nm = 1/3 * max_drift\n",
        "\n",
        "    # Run drift optimization\n",
        "    t0 = time.time()\n",
        "    drift_est = optimize_3d_chunked_better_moving_avg_kd(\n",
        "        segmentation_result.n_segments, sorted_dataset,\n",
        "        sigma_nm=initial_sigma_nm,\n",
        "        target_sigma_nm=target_sigma_nm,\n",
        "        drift_max_nm=max_drift,\n",
        "        drift_max_bound_factor=drift_max_bound_factor,\n",
        "        display_steps=display,\n",
        "        boxcar_width=boxcar_width,\n",
        "        save_intermdiate_results=save_intermediate_results,\n",
        "        force_cpu=force_cpu\n",
        "    )\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    return drift_est\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_ny5iVpdQtb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "66ngAuJlfNMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import a dataset\n",
        "As a first step, you need to import an SMLM dataset that you want to drift correct.  \n",
        "\n",
        "There are TWO options:\n",
        "\n",
        "*   Option 1:   Load a dataset from your local PC.  To upload a dataset from a local drive, click Run for the FIRST code block below.\n",
        "*   Option 2:   Load a dataset from Google drive (faster).  For this option, enter the link to the dataset in the text box, and click Run on the SECOND code block below.\n"
      ],
      "metadata": {
        "id": "5uOTBTo0q7Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2.1 Option 1: Upload a dataset from a local drive\n",
        "from google.colab import files\n",
        "file = files.upload()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "for key in file.keys():\n",
        "  filename = key\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(file[filename]))\n",
        "\n",
        "localizations = np.zeros((len(data['frame']), 4))\n",
        "localizations[:, 0] = np.asarray(data['x [nm]'])\n",
        "localizations[:, 1] = np.asarray(data['y [nm]'])\n",
        "try:\n",
        "  localizations[:, 2] = np.asarray(data['z [nm]'])\n",
        "except:\n",
        "  localizations[:, 2] = 0\n",
        "localizations[:, 3] = np.asarray(data['frame'])\n",
        "frames = np.unique(localizations[:, -1])\n",
        "n_frames = len(frames)\n",
        "print(f\"{filename} import successful, {len(localizations[:, 0])} localizations, {n_frames} frames\")\n",
        "\n",
        "\"\"\"\n",
        "#How to import other formats?\n",
        "#If you use a different data format, modify the lines of code so that you end up\n",
        "#with a numpy array called localizations that has the following dimensions:\n",
        "#\n",
        "#localizations.shape = (number_of_localizations, dataset_dimension+1), where\n",
        "#localizations[:, 0] are the x-coordinates etc. and localizations[:, -1] are the\n",
        "#corresponding frames.\n",
        "\n",
        "from google.colab import files\n",
        "file = files.upload()\n",
        "\n",
        "#Your import code here\n",
        "\n",
        "assert (len(localizations[0,:])==3 or len(localizations[0,:])==4)\n",
        "frames = np.unique(localizations[:, -1])\n",
        "n_frames = len(frames)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iKRUQGQqZnuO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2.2 Option 2: Load a dataset stored on Google drive\n",
        "filename = \"MiFoBio25_test_datasets/test_dataset.csv\" # @param {\"type\":\"string\"}\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path_to_drive_files = \"gdrive/MyDrive/\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(f\"{path_to_drive_files}{filename}\")\n",
        "\n",
        "localizations = np.zeros((len(data['frame']), 4))\n",
        "localizations[:, 0] = np.asarray(data['x [nm]'])\n",
        "localizations[:, 1] = np.asarray(data['y [nm]'])\n",
        "try:\n",
        "  localizations[:, 2] = np.asarray(data['z [nm]'])\n",
        "except:\n",
        "  pass # just skip z localizations if not present\n",
        "localizations[:, 3] = np.asarray(data['frame'])\n",
        "frames = np.unique(localizations[:, -1])\n",
        "n_frames = len(frames)\n",
        "print(f\"{filename} import successful, {len(localizations[:, 0])} localizations, {n_frames} frames\")"
      ],
      "metadata": {
        "id": "kSrVP_2HCGhW",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pNKFGkSyfVRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Set data segmentation options\n",
        "\n",
        "Set the Maximum drift parameter (**max_drift_nm**): this value will vary from microscope to microscope, and depends on environmental conditions.  The units of this varaible are **nanometers**.\n",
        "* Choose an estimate (guess) of the maximum distance which the sample drifted during the acquisition.\n",
        "* This number does not need to be exact, but it must be  larger than the true drift in the dataset.  \n",
        "* If **max_drift_nm** is set too small, the output of COMET will exhibit artifacts (sharp jumps in the estimated drift).\n",
        "* If **max_drift_nm** is set too large, COMET may take a very long time to run, or may report a memory error.\n",
        "* Recommended setting: for typical SMLM experiments, max_drift = 300nm - 2000nm.\n",
        "\n",
        "Next, choose how the dataset will be segmented into time windows.  The larger the number of windows, the higher the time resolution of the drift estimate.\n",
        "\n",
        "There are three options to segment the localization data:\n",
        "\n",
        "* Option 1: **Choose the number of segments (N_seg):** divide the dataset into **N_seg** parts, each containing an equal number of localizations.\n",
        "* Option 2: **Choose the number of localizations per segment (N_loc):** divide the dataset into equal parts, each containing **N_loc** localizations.\n",
        "* Option 3: **Choose the number of camera frames per time window (N_frm):** divide the dataset into equal-length time segments, with **N_frm** per segment.  Note that if a segment contains zero localizations, the algorithm will fail.\n",
        "\n",
        "Depending on which option is chosen, the **segmentation_parameter** corresponds to either **N_seg**, **N_loc**, or **N_frm**.\n",
        "\n",
        "After running the segmentation code, the number of localization pairs (**N_pairs**) will be calculated.  If this number is too high for the CPU/GPU, a warning message is displayed.  In this case, the **downsampling slider** may be used to downsample the data, and the segmentation code can be run again to reduce **N_pairs** to a value which is compatible with the computer.  For Google Colab use, this number should be smaller than 250 x 10^6.\n"
      ],
      "metadata": {
        "id": "T_qpNvx5f39S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.1 Run data segmentation\n",
        "\n",
        "import numpy as np\n",
        "from typing import Optional, Dict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def estimate_pairs(coordinates):\n",
        "  for i in range(len(coordinates[0])):\n",
        "    coordinates[:, i] -= np.min(coordinates[:, i])\n",
        "  coordinates = np.array(np.floor(coordinates / max_drift_nm), dtype=int)\n",
        "  coordinates = np.array(list(map(tuple, coordinates)))\n",
        "  sort_indices = np.lexsort(coordinates.T)# get the unique tuples and their counts\n",
        "  unique_tuples, counts = np.unique(coordinates[sort_indices], axis=0, return_counts=True)\n",
        "  # get the indices of the similar tuples\n",
        "  similar_indices = np.split(sort_indices, np.cumsum(counts[:-1]))\n",
        "  idx_i = []\n",
        "  idx_j = []\n",
        "  pair_idc_estimate = 0\n",
        "  for i in range(len(similar_indices)):\n",
        "    n_elements = len(similar_indices[i])\n",
        "    pair_idc_estimate += n_elements * (n_elements - 1)\n",
        "  rounded = round(pair_idc_estimate,-4)\n",
        "  print(f\"{int(rounded):,d} pairs.\")\n",
        "  if rounded > 100000000:\n",
        "    print(\"This mustn't exceed 250 million! Otherwise use the downsampling slider to reduce \")\n",
        "  elif rounded < 100000:\n",
        "    print(\"Usually in the range of millions, tens of millions up to 100 million. Potentially try increasing the max drift.\")\n",
        "#@title 4.1 Set Maximum Drift\n",
        "max_drift_nm = 2000#@param {type:\"number\"}\n",
        "max_drift_nm = float(max_drift_nm)\n",
        "\n",
        "# Example inputs (replace these with your actual vars in Colab):\n",
        "segmentation_method = \"segments by number of locs per window\" # @param [\"segment by number of time windows\",\"segments by number of locs per window\",\"segment by number of frames per window\"]\n",
        "segmentation_parameter = 500                                # @param {type:\"integer\"}\n",
        "\n",
        "downsampling = 1.0  # @param {type:\"slider\", min:0.025, max:1, step:0.025}\n",
        "\n",
        "# STEP A: pull out the 1‐D array of frame‐indices (must be ints)\n",
        "loc_frames = localizations[:, -1].astype(int)\n",
        "\n",
        "# STEP B: map the old‐style string → “mode” integer for the new API\n",
        "mode_map = {\n",
        "    \"segment by number of time windows\":          0,   # → segment_by_num_windows\n",
        "    \"segments by number of locs per window\":      1,   # → segment_by_num_locs_per_window\n",
        "    \"segment by number of frames per window\":     2,   # → segment_by_frame_windows\n",
        "}\n",
        "if segmentation_method not in mode_map:\n",
        "    raise ValueError(f\"No such segmentation method: {segmentation_method!r}\")\n",
        "min_frame = np.min(loc_frames)\n",
        "max_frame = np.max(loc_frames)\n",
        "seg_mode = mode_map[segmentation_method]\n",
        "if seg_mode == 0:\n",
        "  mean_locs_per_frame = len(loc_frames) // segmentation_parameter\n",
        "  max_locs_per_segment = int(downsampling * mean_locs_per_frame)\n",
        "elif seg_mode == 1:\n",
        "  max_locs_per_segment = int(downsampling * segmentation_parameter)\n",
        "else:\n",
        "  mean_locs_per_frame = len(loc_frames) * segmentation_parameter / len(np.unique(loc_frames))\n",
        "  max_locs_per_segment = int(downsampling * mean_locs_per_frame)\n",
        "print(f\"Using on average {max_locs_per_segment} locs per time window will lead to approximately:\")\n",
        "seg_result = segmentation_wrapper(loc_frames, segmentation_parameter,\n",
        "                                  mode_map[segmentation_method],\n",
        "                                  max_locs_per_segment=max_locs_per_segment,\n",
        "                                  return_param_dict=True)\n",
        "estimate_pairs(localizations[seg_result.loc_valid, :-1].copy())"
      ],
      "metadata": {
        "id": "h0-QhOGEgfIc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Cjl1s3xJ1Mve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Run the COMET algorithm\n",
        "\n",
        "Run the next code block to use COMET to estimate the drift of your segmented localizations."
      ],
      "metadata": {
        "id": "WDoCWop1iYGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4.1 Run COMET\n",
        "t = time.time()\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "drift = comet_run_kd(localizations, seg_result,\n",
        "                     max_drift=max_drift_nm)\n",
        "drift = drift.reshape(seg_result.n_segments, 3)\n",
        "\n",
        "print(f\"algortihm done in {np.round(time.time()-t)}s\")"
      ],
      "metadata": {
        "id": "7bKU-_yyuqF2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PbmXS2gs0-Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Display and download the results\n",
        "\n",
        "The COMET algorithm is complete!  Below are the options to inspect the results and download them for later use."
      ],
      "metadata": {
        "id": "e-gR_2_JmFos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.1 Plot COMET drift estimate\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import CubicSpline\n",
        "#interpolate_drift = True #@param {type:\"boolean\"}\n",
        "#interpolate_drift = True\n",
        "dim = len(drift[0,:])\n",
        "drift_spline = np.zeros((max_frame+1, dim))\n",
        "for i in range(dim):\n",
        "    spline_model = CubicSpline(seg_result.center_frames, drift[:, i],\n",
        "                               bc_type='natural')\n",
        "    full_frames = np.arange((max_frame+1))\n",
        "    drift_spline[:, i] = spline_model(full_frames)\n",
        "plt.plot(full_frames, drift_spline[:, 0]-drift_spline[:,0].mean())\n",
        "plt.plot(full_frames, drift_spline[:, 1]-drift_spline[:,1].mean())\n",
        "plt.plot(full_frames, drift_spline[:, 2]-drift_spline[:,2].mean())\n",
        "plt.xlabel(\"Frames\")\n",
        "plt.ylabel(\"Drift estimate [nm]\")\n",
        "plt.legend([\"x est.\", \"y est.\", \"z est.\"])"
      ],
      "metadata": {
        "id": "abul_BGJmhgC",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5.2 Save the COMET drift estimate as CSV\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "result = np.zeros((len(drift_spline[:, 0]), 4))\n",
        "result[:, 1] = drift_spline[:, 0]\n",
        "result[:, 2] = drift_spline[:, 1]\n",
        "result[:, 3] = drift_spline[:, 2]\n",
        "result[:, 0] = full_frames\n",
        "header = \"frame,x_nm,y_nm,z_nm\\n\"\n",
        "\n",
        "df = pd.DataFrame(result, columns=[\"frame\", \"x_nm\", \"y_nm\", \"z_nm\"])\n",
        "df.to_csv(\"drift_trajectory_result.csv\", index=False, sep=\",\")\n",
        "files.download('drift_trajectory_result.csv')\n"
      ],
      "metadata": {
        "id": "eELMEXlFoK-v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "806e01a6-e26c-4811-9633-30d7c4b7b022",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b6b8e0f1-fe21-465d-b797-bdd1b3dc2474\", \"drift_trajectory_result.csv\", 671446)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5.3 Download drift-corrected SMLM dataset as CSV\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "header = \"frame,x_nm,y_nm,z_nm\\n\"\n",
        "\n",
        "dataset_corrected = np.zeros_like(localizations)\n",
        "for i in range(3):\n",
        "  dataset_corrected[:, i+1] = localizations[:, i] - drift_spline[localizations[:, -1].astype(int), i]\n",
        "dataset_corrected[:, 0] = localizations[:, -1]\n",
        "df = pd.DataFrame(dataset_corrected, columns=[\"frame\", \"x [nm]\", \"y [nm]\", \"z [nm]\"])\n",
        "df.to_csv(\"corrected_dataset.csv\", index=False, sep=\",\",\n",
        "              float_format = '%.3f')\n",
        "files.download('corrected_dataset.csv')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ycZS7Pmd96Gb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}