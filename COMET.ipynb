{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HFKNCFyemCYedyRJqoXFzysPZuc4KHvn",
      "authorship_tag": "ABX9TyNcQu8VdYV2aB/6GZe2+zd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpufit/Comet/blob/master/COMET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMET \n",
        "Cost-function Optimized Maximal overlap drift EsTimation\n",
        "\n",
        "COMET is a software package designed to correct drift in single molecule localization (SMLM)\n",
        "datasets with a high spatial and temporal resolution. \n",
        "\n",
        "Check out the [Github repository](https://github.com/gpufit/Comet) for more detailed information.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yniUwNRUY_wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage of this notebook\n",
        "\n",
        "## Step 0: Set up the notebook\n",
        "Before you run the notebook, please ensure that you are logged into your Google account. Go to file-> save a copy in Drive. Now that you have your own copy of this notebook, go to edit -> notebook-settings and activate the gpu hardware-acceleration. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2H3H9JiFbLco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Importing a dataset\n",
        "As a first step, you need to upload and import the SMLM dataset, that you want to drift-correct. \n",
        "With the following lines of code you can import a file from your local computer, assuming it is saved in the Thunderstorm format. "
      ],
      "metadata": {
        "id": "5uOTBTo0q7Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "file = files.upload()\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "for key in file.keys():\n",
        "  filename = key\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(file[filename]))\n",
        "\n",
        "localizations = np.zeros((len(data['frame']), 4))\n",
        "localizations[:, 0] = np.asarray(data['x [nm]'])\n",
        "localizations[:, 1] = np.asarray(data['y [nm]'])\n",
        "localizations[:, 2] = np.asarray(data['z [nm]'])\n",
        "localizations[:, 3] = np.asarray(data['frame'])\n",
        "frames = np.unique(localizations[:, -1])\n",
        "n_frames = len(frames)\n",
        "print(f\"{filename} import successful, {len(localizations[:, 0])} localizations, {n_frames} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "iKRUQGQqZnuO",
        "outputId": "61182c1d-840e-4985-f62e-8e5d8d7086a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f8e56181-e096-4ad9-9e72-c54fc6130c47\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f8e56181-e096-4ad9-9e72-c54fc6130c47\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving image_localizations_no_drift_corr_cropped.csv to image_localizations_no_drift_corr_cropped.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "... alternatively you can directly import them from your google drive using the follwoing code and replacing \"test_dataset.csv\" with your filename"
      ],
      "metadata": {
        "id": "hXeTf6FPB5N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path_to_drive_files = \"gdrive/MyDrive/\"\n",
        "filename = \"test_dataset.csv\"\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "data = pd.read_csv(f\"{path_to_drive_files}{filename}\")\n",
        "\n",
        "localizations = np.zeros((len(data['frame']), 4))\n",
        "localizations[:, 0] = np.asarray(data['x [nm]'])\n",
        "localizations[:, 1] = np.asarray(data['y [nm]'])\n",
        "localizations[:, 2] = np.asarray(data['z [nm]'])\n",
        "localizations[:, 3] = np.asarray(data['frame'])\n",
        "frames = np.unique(localizations[:, -1])\n",
        "n_frames = len(frames)\n",
        "print(f\"{filename} import successful, {len(localizations[:, 0])} localizations, {n_frames} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSrVP_2HCGhW",
        "outputId": "e0f69fa2-8ed1-464c-944c-fcf70de02fba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "test_dataset.csv import successful, 300000 localizations, 14965 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### how to import other formats?\n",
        "If you use a different data format, modify the lines of code so that you end up with a numpy array called localizations that has the following dimensions:\n",
        "\n",
        "localizations.shape = (number_of_localizations, dataset_dimension+1), where localizations[:, 0] are the x-coordinates etc. and localizations[:, -1] are the corresponding frames."
      ],
      "metadata": {
        "id": "XP0-_twMeQq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "file = files.upload()\n",
        "\n",
        "\"\"\"\n",
        "Your import code here\n",
        "\"\"\"\n",
        "assert (len(localizations[0,:])==3 or len(localizations[0,:])==4)\n",
        "frames = np.unique(localizations[:, -1])\n",
        "n_frames = len(frames)"
      ],
      "metadata": {
        "id": "Fwa0zxNuuuTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "5398d052-194e-4677-9596-2955f27ab45a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c031402a-a2c9-46a0-b52a-a8489a66cf80\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c031402a-a2c9-46a0-b52a-a8489a66cf80\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Segment dataset\n",
        "In the next step, the dataset is going to be segmented. Currently we provide 3 segmentation methods, but of cause feel free to try out your own by modifying the following lines of code.\n",
        "\n",
        "1) **segment by number of time windows**: \n",
        "\n",
        "this divides the dataset in equal parts containing a fixed \n",
        "number of localizations, you can specify the number of parts. \n",
        "\n",
        "2) **segments by number of locs per window**\n",
        "\n",
        "similar to option 1)\n",
        "but here you have to specify the number of localizations per \n",
        "window and the resulting number of segments is calculated. \n",
        "\n",
        "3) **segment by number of frames per window**:\n",
        "\n",
        "this will use the information provided by the dataset and split the dataset in unequal parts, where each part contains all the localizations witin a number of frames that you specify.\n",
        "\n",
        "Run the next few lines to first define the segmentation methods and the code after to apply one of the segmentation methods to the earlier imported localizations. The default here is method 1 with 200 segments, feel free to to play around with the segmentation methods and try to find the optimal parameters. \n",
        "\n",
        "-> Increasing the number of segments will increase the time resolution of the drift estimation, but makes the estimation less robust or in extreme cases can lead to wrong estimates. \n"
      ],
      "metadata": {
        "id": "T_qpNvx5f39S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_method = 'segment by number of time windows' #@param [\"'segment by number of time windows'\", \"'segments by number of locs per window'\", \"'segment by number of frames per window'\"] {type:\"raw\", allow-input: true}\n",
        "segmentation_parameter = 300 #@param{type:\"integer\"}\n",
        "\n",
        "def segment_by_num_windows(locs_nm, n_windows, return_n_segments=False):\n",
        "    n_windows = int(n_windows)\n",
        "    n_locs = len(locs_nm[:, 0])\n",
        "    n_locs_per_window = int(np.ceil(n_locs/n_windows))\n",
        "    if return_n_segments:\n",
        "        return segment_by_num_locs_per_window(locs_nm, n_locs_per_window), n_windows\n",
        "    else:\n",
        "        return segment_by_num_locs_per_window(locs_nm, n_locs_per_window)\n",
        "\n",
        "\n",
        "def segment_by_num_locs_per_window(locs_nm, n_locs_per_window, return_n_segments=False):\n",
        "    locs_nm[:, -1] = 0\n",
        "    n_locs_per_window = int(n_locs_per_window)\n",
        "    n_locs = len(locs_nm[:, 0])\n",
        "    n_windows = int(np.ceil(n_locs/n_locs_per_window))\n",
        "    for i in range(n_windows-1):\n",
        "        locs_nm[(i+1)*n_locs_per_window:, -1] += 1\n",
        "    if return_n_segments:\n",
        "        return locs_nm, n_windows\n",
        "    else:\n",
        "        return locs_nm\n",
        "\n",
        "\n",
        "def segment_by_frame_windows(locs_nm, n_frames_per_seg, return_n_segments=False):\n",
        "    n_frames = int(np.max(locs_nm[:, -1]) - np.min(locs_nm[:, -1]))\n",
        "    n_segments = int(np.ceil(n_frames/n_frames_per_seg))\n",
        "    locs_nm[:, -1] = np.floor((locs_nm[:, -1] - np.min(locs_nm[:, -1]))/n_frames_per_seg)\n",
        "    if return_n_segments:\n",
        "        return locs_nm, n_segments\n",
        "    else:\n",
        "        return locs_nm\n",
        "      \n",
        "if segmentation_method == \"segment by number of time windows\":\n",
        "  localizations, n_segments = segment_by_num_windows(localizations, segmentation_parameter, True)\n",
        "elif segmentation_method == \"segments by number of locs per window\":\n",
        "  localizations, n_segments = segment_by_num_locs_per_window(localizations, segmentation_parameter, True)\n",
        "elif segmentation_method == \"segment by number of frames per window\":\n",
        "  localizations, n_segments = segment_by_frame_windows(localizations, segmentation_parameter, True)\n",
        "else:\n",
        "  print(\"Error: no such segmentation method\")\n",
        "#print(f\"{np.min(localizations[:, -1])}, {np.max(localizations[:, -1])}\")\n",
        "print(f\"segmentation into {n_segments} segments done\")"
      ],
      "metadata": {
        "id": "h0-QhOGEgfIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2b6f0b-2199-4764-8fe6-60dc181b938f",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation into 300 segments done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Run the algorithm \n",
        "\n",
        "The next few lines show the heart of the comet project: the drift estimation algortihm. There is a 2D- and 3D-version of the algortihm for 2D- and 3D-datasets respectively. Run the next block of code to define the functions and the block of code after to estimate the drift of your segmented localizations. There you will find a variable called max_drift_nm with is 300 nm by default. If you're able to estimate the maximum drift in your dataset feel free to adjust the parameter, but be aware, that it's directly proportional to the calculation time of the algortihm. "
      ],
      "metadata": {
        "id": "WDoCWop1iYGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "import math \n",
        "from scipy.optimize import minimize\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "def pair_indices_lex_floor(coordinates, distance):\n",
        "    for i in range(len(coordinates[0])):\n",
        "        coordinates[:, i] -= np.min(coordinates[:, i])\n",
        "    coordinates = np.array(np.floor(coordinates / distance), dtype=int)\n",
        "    coordinates = np.array(list(map(tuple, coordinates)))\n",
        "\n",
        "    sort_indices = np.lexsort(coordinates.T)\n",
        "\n",
        "    # get the unique tuples and their counts\n",
        "    unique_tuples, counts = np.unique(coordinates[sort_indices], axis=0, return_counts=True)\n",
        "\n",
        "    # get the indices of the similar tuples\n",
        "    similar_indices = np.split(sort_indices, np.cumsum(counts[:-1]))\n",
        "\n",
        "    idx_i = []\n",
        "    idx_j = []\n",
        "    for i in range(len(similar_indices)):\n",
        "        if len(similar_indices[i]) > 1:\n",
        "            idc = similar_indices[i]\n",
        "            for j in range(len(idc)-1):\n",
        "                tmp_n_entries = (len(idc) - j) - 1\n",
        "                idx_i.append(np.repeat(np.int32(idc[j]), tmp_n_entries))\n",
        "                idx_j.append(idc[np.arange(tmp_n_entries) + (j+1)])\n",
        "    return idx_i, idx_j\n",
        "\n",
        "@cuda.jit\n",
        "def cost_function_full_3d_chunked(d_locs_time, start_idx, chunk_size, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val, d_val_sum, d_deri=np.array([[]]),\n",
        "                          d_locs_coords=np.array([[]]), mu=np.array([[]])):\n",
        "    # Thread id in a 1D block\n",
        "    tx = cuda.threadIdx.x\n",
        "    # Block id in a 1D grid\n",
        "    ty = cuda.blockIdx.x\n",
        "    # Block width, i.e. number of threads per block\n",
        "    bw = cuda.blockDim.x\n",
        "    # Compute flattened index inside the array\n",
        "    pos = tx + ty * bw\n",
        "    if pos < chunk_size:\n",
        "        d_val[pos] = (math.exp(1)**(-(((d_locs_coords[d_idx_i[pos+start_idx], 0] - mu[d_locs_time[d_idx_i[pos+start_idx]], 0]) -\n",
        "                                       (d_locs_coords[d_idx_j[pos+start_idx], 0] - mu[d_locs_time[d_idx_j[pos+start_idx]], 0]))**2 +\n",
        "                                      ((d_locs_coords[d_idx_i[pos+start_idx], 1] - mu[d_locs_time[d_idx_i[pos+start_idx]], 1]) -\n",
        "                                       (d_locs_coords[d_idx_j[pos+start_idx], 1] - mu[d_locs_time[d_idx_j[pos+start_idx]], 1]))**2 +\n",
        "                                      ((d_locs_coords[d_idx_i[pos+start_idx], 2] - mu[d_locs_time[d_idx_i[pos+start_idx]], 2]) -\n",
        "                                       (d_locs_coords[d_idx_j[pos+start_idx], 2] - mu[d_locs_time[d_idx_j[pos+start_idx]], 2]))**2) /\n",
        "                                    ((d_sigma*d_sigma_factor) ** 2)))\n",
        "        cuda.atomic.add(d_deri, (d_locs_time[d_idx_i[pos+start_idx]], 0), d_val[pos] * 2.*(d_locs_coords[d_idx_i[pos+start_idx], 0] -\n",
        "                                                                                 d_locs_coords[d_idx_j[pos+start_idx], 0] +\n",
        "                                                                                 mu[d_locs_time[d_idx_j[pos+start_idx]], 0] -\n",
        "                                                                                 mu[d_locs_time[d_idx_i[pos+start_idx]], 0]) /\n",
        "                        (d_sigma*d_sigma_factor) ** 2)\n",
        "        cuda.atomic.add(d_deri, (d_locs_time[d_idx_i[pos+start_idx]], 1), d_val[pos] * 2. * (\n",
        "                    d_locs_coords[d_idx_i[pos+start_idx], 1] - d_locs_coords[d_idx_j[pos+start_idx], 1] + mu[d_locs_time[d_idx_j[pos+start_idx]], 1] -\n",
        "                    mu[d_locs_time[d_idx_i[pos+start_idx]], 1]) / (d_sigma * d_sigma_factor) ** 2)\n",
        "        cuda.atomic.add(d_deri, (d_locs_time[d_idx_i[pos+start_idx]], 2), d_val[pos] * 2. * (\n",
        "                    d_locs_coords[d_idx_i[pos+start_idx], 2] - d_locs_coords[d_idx_j[pos+start_idx], 2] + mu[d_locs_time[d_idx_j[pos+start_idx]], 2] -\n",
        "                    mu[d_locs_time[d_idx_i[pos+start_idx]], 2]) / (d_sigma * d_sigma_factor) ** 2)\n",
        "        cuda.atomic.add(d_val_sum, 0, d_val[pos])\n",
        "        d_val[pos] = 0\n",
        "\n",
        "\n",
        "def cuda_wrapper_chunked_3d(mu, d_locs_coords, d_locs_time, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val, d_deri,\n",
        "                 chunk_size):\n",
        "    val = 0\n",
        "    d_val_sum = cuda.to_device(np.zeros(1, dtype=np.float64))\n",
        "    #mu = np.append([0, 0, 0], np.asarray(mu))\n",
        "    mu = np.asarray(mu.reshape(int(mu.size / 3), 3), dtype=np.float64)\n",
        "    n_chunks = int(np.ceil(d_idx_i.size/chunk_size))\n",
        "    for i in range(n_chunks-1):\n",
        "        threadsperblock = 256\n",
        "        idc = np.arange(chunk_size) + i*chunk_size\n",
        "        blockspergrid = (chunk_size + (threadsperblock - 1)) // threadsperblock\n",
        "        cost_function_full_3d_chunked[blockspergrid, threadsperblock](d_locs_time, idc[0], chunk_size, d_idx_i, d_idx_j, d_sigma,\n",
        "                                                              d_sigma_factor, d_val, d_val_sum, d_deri, d_locs_coords, mu)\n",
        "        val += d_val_sum.copy_to_host()\n",
        "\n",
        "    n_remaining = d_idx_i.size - (n_chunks - 1) * chunk_size\n",
        "    idc = np.arange(n_remaining) + (n_chunks - 1) * chunk_size\n",
        "    threadsperblock = 256\n",
        "    blockspergrid = (n_remaining + (threadsperblock - 1)) // threadsperblock\n",
        "    cost_function_full_3d_chunked[blockspergrid, threadsperblock](d_locs_time, idc[0], n_remaining, d_idx_i, d_idx_j, d_sigma,\n",
        "                                                          d_sigma_factor, d_val, d_val_sum, d_deri, d_locs_coords, mu)\n",
        "    val += d_val_sum.copy_to_host()[:n_remaining]\n",
        "    deri = d_deri.copy_to_host()\n",
        "    d_deri[:] = 0\n",
        "    return -np.nansum(val), -deri.flatten()\n",
        "\n",
        "\n",
        "def optimize_3d_chunked(n_segments, locs_nm, sigma_nm=30, drift_max_nm=300, sigma_factor=1, threshold_estimator_nm=5,\n",
        "                display_steps=False):\n",
        "    drift_estimate = np.zeros(3 * n_segments)\n",
        "    bounds = []\n",
        "    for i in range(n_segments):\n",
        "        bounds.append(((i + 1) * -drift_max_nm, (i + 1) * drift_max_nm))\n",
        "        bounds.append(((i + 1) * -drift_max_nm, (i + 1) * drift_max_nm))\n",
        "        bounds.append(((i + 1) * -drift_max_nm, (i + 1) * drift_max_nm))\n",
        "    bounds = np.asarray(bounds)\n",
        "\n",
        "    idx_i, idx_j = pair_indices_lex_floor(locs_nm[:, :3].copy(), drift_max_nm)\n",
        "\n",
        "    d_locs_coords = cuda.to_device(np.asarray(locs_nm[:, :3], dtype=np.float32).copy())\n",
        "    d_locs_time = cuda.to_device(np.asarray(locs_nm[:, 3], dtype=np.int32).copy())\n",
        "    idx_i = np.asarray(np.concatenate(idx_i).ravel(), dtype=np.int32)\n",
        "    idx_j = np.asarray(np.concatenate(idx_j).ravel(), dtype=np.int32)\n",
        "    if len(idx_i) * 32 / 8 > 1000000000:  # array size in the GB range -> use mapped memory\n",
        "        print(\"huge number of pairs: using mapped memory\")\n",
        "        d_idx_i = cuda.mapped_array_like(idx_i, wc=True)\n",
        "        d_idx_j = cuda.mapped_array_like(idx_j, wc=True)\n",
        "        d_idx_i[:] = idx_i\n",
        "        d_idx_j[:] = idx_j\n",
        "    else:\n",
        "        d_idx_i = cuda.to_device(idx_i)\n",
        "        d_idx_j = cuda.to_device(idx_j)\n",
        "    #del idx_i\n",
        "    #del idx_j\n",
        "\n",
        "    d_sigma = np.float64(sigma_nm)\n",
        "    chunk_size = int(1E7)\n",
        "    d_val = cuda.to_device(np.zeros(int(chunk_size)))\n",
        "    deri = np.zeros((n_segments, 3), dtype=np.float64)\n",
        "    d_deri = cuda.to_device(deri)\n",
        "    print(\"preparation done: now starting the algorithm\")\n",
        "    optimization_done = 0\n",
        "    fails = 0\n",
        "    drift_estimate_gradient = np.inf\n",
        "    while optimization_done == 0:\n",
        "        d_sigma_factor = np.float64(sigma_factor)\n",
        "        result = minimize(cuda_wrapper_chunked_3d, drift_estimate, method='L-BFGS-B', options={'disp': display_steps},\n",
        "                          bounds=bounds, jac=True,\n",
        "                          args=(d_locs_coords, d_locs_time, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val,\n",
        "                                d_deri, chunk_size))\n",
        "        if np.mean((result['x']-drift_estimate)**2) > drift_estimate_gradient and result['success']:\n",
        "            optimization_done = 1\n",
        "        elif result['success']:\n",
        "            sigma_factor = sigma_factor / 1.5\n",
        "            drift_estimate = result['x']\n",
        "            drift_estimate_gradient = np.mean((result['x']-drift_estimate)**2)\n",
        "        else:\n",
        "            fails += 1\n",
        "            if fails > 2:\n",
        "                print(\"restarting with bigger sigma_factor\")\n",
        "                sigma_factor = sigma_factor * 2\n",
        "            if fails > 5:\n",
        "                raise RuntimeError('L-BFGS-B Optimization failed')\n",
        "\n",
        "    return drift_estimate\n",
        "\n",
        "@cuda.jit\n",
        "def cost_function_full_2d_chunked(d_locs_time, start_idx, chunk_size, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val,\n",
        "                                  d_val_sum, d_deri=np.array([[]]),\n",
        "                                  d_locs_coords=np.array([[]]), mu=np.array([[]])):\n",
        "    # Thread id in a 1D block\n",
        "    tx = cuda.threadIdx.x\n",
        "    # Block id in a 1D grid\n",
        "    ty = cuda.blockIdx.x\n",
        "    # Block width, i.e. number of threads per block\n",
        "    bw = cuda.blockDim.x\n",
        "    # Compute flattened index inside the array\n",
        "    pos = tx + ty * bw\n",
        "    if pos < chunk_size:\n",
        "        d_val[pos] = (math.exp(1) ** (\n",
        "                    -(((d_locs_coords[d_idx_i[pos + start_idx], 0] - mu[d_locs_time[d_idx_i[pos + start_idx]], 0]) -\n",
        "                       (d_locs_coords[d_idx_j[pos + start_idx], 0] - mu[\n",
        "                           d_locs_time[d_idx_j[pos + start_idx]], 0])) ** 2 +\n",
        "                      ((d_locs_coords[d_idx_i[pos + start_idx], 1] - mu[d_locs_time[d_idx_i[pos + start_idx]], 1]) -\n",
        "                       (d_locs_coords[d_idx_j[pos + start_idx], 1] - mu[\n",
        "                           d_locs_time[d_idx_j[pos + start_idx]], 1])) ** 2) /\n",
        "                    ((d_sigma * d_sigma_factor) ** 2)))\n",
        "        cuda.atomic.add(d_deri, (d_locs_time[d_idx_i[pos + start_idx]], 0),\n",
        "                        d_val[pos] * 2. * (d_locs_coords[d_idx_i[pos + start_idx], 0] - d_locs_coords[\n",
        "                            d_idx_j[pos + start_idx], 0] +\n",
        "                                           mu[d_locs_time[d_idx_j[pos + start_idx]], 0] - mu[\n",
        "                                               d_locs_time[d_idx_i[pos + start_idx]], 0])\n",
        "                        / (d_sigma * d_sigma_factor) ** 2)\n",
        "        cuda.atomic.add(d_deri, (d_locs_time[d_idx_i[pos + start_idx]], 1), d_val[pos] * 2. * (\n",
        "                d_locs_coords[d_idx_i[pos + start_idx], 1] - d_locs_coords[d_idx_j[pos + start_idx], 1] + mu[\n",
        "            d_locs_time[d_idx_j[pos + start_idx]], 1] -\n",
        "                mu[d_locs_time[d_idx_i[pos + start_idx]], 1]) / (d_sigma * d_sigma_factor) ** 2)\n",
        "        cuda.atomic.add(d_val_sum, 0, d_val[pos])\n",
        "        d_val[pos] = 0\n",
        "\n",
        "\n",
        "def cuda_wrapper_chunked(mu, d_locs_coords, d_locs_time, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val, d_deri,\n",
        "                         chunk_size):\n",
        "    #t = time.time()\n",
        "    val = 0\n",
        "    d_val_sum = cuda.to_device(np.zeros(1, dtype=np.float64))\n",
        "    #mu = np.append([0, 0], np.asarray(mu))\n",
        "    mu = np.asarray(mu.reshape(int(mu.size / 2), 2), dtype=np.float64)\n",
        "    n_chunks = int(np.ceil(d_idx_i.size / chunk_size))\n",
        "    #print(f\"Preparing GPU calculation in {time.time() - t}s, number of chunks: {n_chunks}\")\n",
        "\n",
        "    for i in range(n_chunks - 1):\n",
        "        #t = time.time()\n",
        "        threadsperblock = 256\n",
        "        idc = np.arange(chunk_size) + i * chunk_size\n",
        "        blockspergrid = (d_idx_j.size + (threadsperblock - 1)) // threadsperblock\n",
        "        cost_function_full_2d_chunked[blockspergrid, threadsperblock](d_locs_time, idc[0], chunk_size, d_idx_i, d_idx_j,\n",
        "                                                                      d_sigma,\n",
        "                                                                      d_sigma_factor, d_val, d_val_sum,\n",
        "                                                                      d_deri, d_locs_coords, mu)\n",
        "        #print(f\"gpu calculation step {i+1}/{n_chunks} done in {time.time() - t}\")\n",
        "        val += d_val_sum.copy_to_host()\n",
        "    n_remaining = d_idx_i.size - (n_chunks - 1) * chunk_size\n",
        "    idc = np.arange(n_remaining) + (n_chunks - 1) * chunk_size\n",
        "    threadsperblock = 256\n",
        "    blockspergrid = (n_remaining + (threadsperblock - 1)) // threadsperblock\n",
        "    cost_function_full_2d_chunked[blockspergrid, threadsperblock](d_locs_time, idc[0], n_remaining, d_idx_i, d_idx_j,\n",
        "                                                                  d_sigma, d_sigma_factor, d_val, d_val_sum, d_deri,\n",
        "                                                                  d_locs_coords, mu)\n",
        "    #t = time.time()\n",
        "    val += d_val_sum.copy_to_host()[:n_remaining]\n",
        "    deri = d_deri.copy_to_host()\n",
        "    #print(f\"time to copy derivatives to host {time.time() - t}\")\n",
        "    d_deri[:] = 0\n",
        "    return -np.nansum(val), -deri.flatten()\n",
        "\n",
        "\n",
        "def optimize_2d_chunked(n_segments, locs_nm, sigma_nm=30, drift_max_nm=300, sigma_factor=1, threshold_estimator_nm=5,\n",
        "                        display_steps=False):\n",
        "    drift_estimate = np.zeros(2 * n_segments)\n",
        "    bounds = []\n",
        "    for i in range(n_segments):\n",
        "        bounds.append(((i + 1) * -drift_max_nm, (i + 1) * drift_max_nm))\n",
        "        bounds.append(((i + 1) * -drift_max_nm, (i + 1) * drift_max_nm))\n",
        "\n",
        "    idx_i, idx_j = pair_indices_lex_floor(locs_nm[:, :2].copy(), drift_max_nm)\n",
        "\n",
        "    d_locs_coords = cuda.to_device(np.asarray(locs_nm[:, :2], dtype=np.float32).copy())\n",
        "    d_locs_time = cuda.to_device(np.asarray(locs_nm[:, 2].astype(int), dtype=np.int32).copy())\n",
        "    idx_i = np.asarray(np.concatenate(idx_i).ravel(), dtype=np.int32)\n",
        "    idx_j = np.asarray(np.concatenate(idx_j).ravel(), dtype=np.int32)\n",
        "    if len(idx_i) * 32 / 8 > 1000000000:  # array size in the GB range -> use mapped memory\n",
        "        print(\"huge number of pairs: using mapped memory\")\n",
        "        d_idx_i = cuda.mapped_array_like(idx_i, wc=True)\n",
        "        d_idx_j = cuda.mapped_array_like(idx_j, wc=True)\n",
        "        d_idx_i[:] = idx_i\n",
        "        d_idx_j[:] = idx_j\n",
        "    else:\n",
        "        d_idx_i = cuda.to_device(idx_i)\n",
        "        d_idx_j = cuda.to_device(idx_j)\n",
        "    d_sigma = np.float64(sigma_nm)\n",
        "    chunk_size = int(1E7)\n",
        "    d_val = cuda.to_device(np.zeros(chunk_size))\n",
        "    deri = np.zeros((n_segments, 2), dtype=np.float64)\n",
        "    d_deri = cuda.to_device(deri)\n",
        "    print(\"preparation done: now starting the algorithm\")\n",
        "    optimization_done = 0\n",
        "    fails = 0\n",
        "    drift_estimate_gradient = np.inf\n",
        "    while optimization_done == 0:\n",
        "        d_sigma_factor = np.float64(sigma_factor)\n",
        "        result = minimize(cuda_wrapper_chunked, drift_estimate, method='L-BFGS-B', options={'disp': display_steps},\n",
        "                          bounds=bounds, jac=True,\n",
        "                          args=(d_locs_coords, d_locs_time, d_idx_i, d_idx_j, d_sigma, d_sigma_factor, d_val, d_deri,\n",
        "                                chunk_size))\n",
        "        if np.mean((result['x'] - drift_estimate) ** 2) > drift_estimate_gradient and result['success']:\n",
        "            optimization_done = 1\n",
        "        elif result['success']:\n",
        "            sigma_factor = sigma_factor / 1.5\n",
        "            drift_estimate = result['x']\n",
        "            drift_estimate_gradient = np.mean((result['x'] - drift_estimate) ** 2)\n",
        "        else:\n",
        "            fails += 1\n",
        "            if fails > 2:\n",
        "                print(\"restarting with bigger sigma_factor\")\n",
        "                sigma_factor = sigma_factor * 2\n",
        "            if fails > 5:\n",
        "                raise RuntimeError('L-BFGS-B Optimization failed')\n",
        "    return drift_estimate\n",
        "\n",
        "\n",
        "max_drift_nm = 300#@param {type:\"number\"}\n",
        "max_drift_nm = float(max_drift_nm)\n",
        "dataset_dimension = 3#@param [2,3]{type:\"raw\"}\n",
        "initial_gaussian_scale_nm = 30#@param {type:\"number\"}\n",
        "t = time.time()\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "if dataset_dimension == 3:\n",
        "  drift = optimize_3d_chunked(n_segments, localizations,\n",
        "                              sigma_nm = initial_gaussian_scale_nm,\n",
        "                              drift_max_nm=max_drift_nm,\n",
        "                              display_steps=False)\n",
        "  drift = drift.reshape(n_segments, 3)\n",
        "else:\n",
        "  drift = optimize_2d_chunked(n_segments, localizations,\n",
        "                              drift_max_nm=max_drift_nm,\n",
        "                              sigma_nm = initial_gaussian_scale_nm,\n",
        "                              display_steps=False)\n",
        "  drift = drift.reshape(n_segments, 2)\n",
        "print(f\"algortihm done in {np.round(time.time()-t)}s\")"
      ],
      "metadata": {
        "id": "_dtovWDqjKYk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Displaying and downloading the results\n",
        "\n",
        "In principle the algorithm is done at this point, now we want to look at the results and possibly download them for later use. \n",
        "To display the result we will use the next block of code, with shows creates a plot of the resulting drift. "
      ],
      "metadata": {
        "id": "e-gR_2_JmFos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "from scipy.interpolate import CubicSpline \n",
        "interpolate_drift = False #@param {type:\"boolean\"}\n",
        "if not interpolate_drift:\n",
        "  plt.plot(np.arange(n_segments)/n_segments*n_frames, drift[:,0])\n",
        "  plt.plot(np.arange(n_segments)/n_segments*n_frames, drift[:,1])\n",
        "  if dataset_dimension == 3:\n",
        "    plt.plot(np.arange(n_segments)/n_segments*n_frames, drift[:,2])\n",
        "  plt.xlabel(\"Segments\")\n",
        "  plt.ylabel(\"Drift estimate [nm]\")\n",
        "else: \n",
        "  dim = len(drift[0,:])\n",
        "  drift_spline = np.zeros((n_frames, dim))\n",
        "  for i in range(dim):\n",
        "      spline_model = CubicSpline(np.arange(n_segments), drift[:, i])\n",
        "      drift_spline[:, i] = spline_model(np.arange(n_frames)/n_frames*n_segments)\n",
        "  plt.plot(frames, drift_spline[:, 0])\n",
        "  plt.plot(frames, drift_spline[:, 1])\n",
        "  if dataset_dimension ==3:\n",
        "    plt.plot(frames, drift_spline[:, 2])\n",
        "  plt.xlabel(\"Frames\")\n",
        "  plt.ylabel(\"Drift estimate [nm]\")\n",
        "if dataset_dimension == 2:\n",
        "  plt.legend([\"x est.\", \"y est.\"])\n",
        "else:\n",
        "  plt.legend([\"x est.\", \"y est.\", \"z est.\"])"
      ],
      "metadata": {
        "id": "abul_BGJmhgC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last but not least the next block of code will save the resulting drift in an .csv file, which you can simply find in the directory (folder symbol in upper left corner) after you ran the next lines of code."
      ],
      "metadata": {
        "id": "n6yER1mjnxAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset_dim ==3:\n",
        "  header = \"frame,x_nm,y_nm,z_nm\\n\"\n",
        "else:\n",
        "  header = \"frame,x_nm,y_nm\\n\"\n",
        "file_contents = (header+str(drift).replace(' [', '').replace('[', '').replace('],', ',\\n').replace(']', '')).encode()\n",
        "with open(\"results.csv\", 'wb+') as f:\n",
        "  f.write(file_contents)\n",
        "\n"
      ],
      "metadata": {
        "id": "eELMEXlFoK-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}